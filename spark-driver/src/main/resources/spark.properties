# spark://127.0.0.1:7077 or local
# local[n] is a special value that runs Spark on n threads on the local machine,
# without connecting to a cluster.
spark.master=local[2]

# Spark application name.
spark.application-name=Lambda Architecture with Apache Spark

# Path to distributed library that should be loaded into each worker of a Spark cluster.
spark.distributed-libraries=<spark distributed libraries path should be correct path used on different environments>

# The maximum amount of CPU cores to request for the application from across the cluster (not from each machine).
spark.cores.max=1

# Amount of memory to assign to each executor process
spark.executor.memory=1g

# The largest number of partitions in a parent RDD during distributed shuffle operations.
# For local mode should be equal to number of cores on the local machine.
spark.default.parallelism=4

# Serializer: org.apache.spark.serializer.JavaSerializer (default) or org.apache.spark.serializer.KryoSerializer
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false

# The number of partitions to use when shuffling data for joins or aggregations.
spark.sql.shuffle.partitions=5

# Streaming properties.

# Interval in seconds to trigger new data into stream, i.e. interval between mini batches.
spark.streaming.batch.duration.seconds=15

# You can also run SQL queries on tables defined on streaming data from a different thread
# (that is, asynchronous to the running StreamingContext).
# Just make sure that you set the StreamingContext to remember a sufficient amount of streaming data such that the query can run.
# Otherwise the StreamingContext, which is unaware of the any asynchronous SQL queries,
# will delete off old streaming data before the query can complete.
spark.streaming.remember.duration.seconds=6000

# Whether start Spark streaming context as it should be on production or
# just create it and let tests configure it for own purposes.
spark.streaming.context.enable=true

# Directory used for check pointing.
spark.streaming.checkpoint.directory=/tmp/checkpoint

# Whether enable write ahead logging or not.
spark.streaming.receiver.writeAheadLog.enable=true

# Directory used as backup plan for streaming when Twitter is not available.
spark.streaming.directory=<Spark streaming directory should be correct path used on different environments>

# Whether use file stream instead of real Twitter.
# Main purpose - is backup plan when there is no Internet connection.
spark.streaming.file.stream.enable=false